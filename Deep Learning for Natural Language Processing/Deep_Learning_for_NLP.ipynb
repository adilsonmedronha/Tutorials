{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Natural Language Processing\n",
    "\n",
    "***This is my notes of 7-day Mini-Course (created by Jason Brownlee) <br>*** *May have some classmates insights*\n",
    "\n",
    "This crash course is broken down into 7 lessons.\n",
    "\n",
    "Below are 7 lessons that will get you started and productive with deep learning for natural language processing in Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lesson 01: [Deep Learning and Natural Language](#01)\n",
    "* Lesson 02: [Cleaning Text Data](#02)\n",
    "* Lesson 03: [Bag-of-Words Model](#03)\n",
    "* Lesson 04: [Word Embedding Representation](#04)\n",
    "* Lesson 05: [Learned Embedding](#05)\n",
    "* Lesson 06: [Classifying Text](#06)\n",
    "* Lesson 07: [Movie Review Sentiment Analysis Project](#07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='01'></a>\n",
    "## Deep Learning and Natural Language ##\n",
    "\n",
    "* NPL: automatic manipulation of natural language, like speech and text, by software.\n",
    "* Deep Learning is a subfield of ML concerned with algorithms inspired by the structure and function of the brain (ANN).\n",
    "* A nice benefit of DP is the ability to perfom automatic feature extraction from raw data (**feature learning**).\n",
    "\n",
    "10 impressive applications of deep learning\n",
    "1. **Automatic Colorization of Black and White Images** <br>\n",
    "    Generally this approach involves the use of very large convolutional neural networks and <br> supervised layers that recreate the image with the addition of color. <br>\n",
    "    The research [Learning Representations for Automatic Colorization]() <br>\n",
    "    strives to make colorization  cost-effective and less-time consuming. <br>\n",
    "\n",
    "    **used dataset**: [ImageNet](https://paperswithcode.com/dataset/imagenet) \n",
    "    \n",
    "    *E.g., second the autor, using [Hand-colouring](https://www.reddit.com/r/Colorization/), this photo took approximately 1h30 to be done (colorized):*  <br>\n",
    "    \n",
    "    <img src=\"https://raw.githubusercontent.com/adilsonmedronha/Tutorials/main/Deep%20Learning%20for%20Natural%20Language%20Processing/images/1_reddit_post_ww1_German_soldier_colorization.png\" width=\"250\" height=\"250\" />\n",
    "    \n",
    "    With multiple applications that can benefit from automatic colorization:\n",
    "    * Historical photographs and videos\n",
    "    * Artist assistance \n",
    "    * papers: [1](https://github.com/adilsonmedronha/Tutorials/blob/main/Deep%20Learning%20for%20Natural%20Language%20Processing/papers/1_Colorful_Image_Colorization_1603-08511.pdf), [2](https://github.com/adilsonmedronha/Tutorials/blob/main/Deep%20Learning%20for%20Natural%20Language%20Processing/papers/1_Learning_Representations_for_Automatic_Colorization_1603-06668.pdf) \n",
    "2. **Automatically Adding Sounds to Silent Movies** <br>\n",
    "A deep learning model associates the video frames with a database of pre-rerecorded sounds <br> in order to select a sound to play that best matches what is happening in the scene.\n",
    "    * [Video demonstration](https://youtu.be/0FW99AQmMc8) \n",
    "    * [Paper](https://github.com/adilsonmedronha/Tutorials/blob/main/Deep%20Learning%20for%20Natural%20Language%20Processing/papers/2_Visually_Indicated_Sounds_1512-08512.pdf)\n",
    "3. **Automatic Handwriting Generation** <br>\n",
    "    This is an interesting task, where a corpus of text is learned and from this model new text is generated, word-by-word or character-by-character.\n",
    "    * Papers: [1](https://github.com/adilsonmedronha/Tutorials/blob/main/Deep%20Learning%20for%20Natural%20Language%20Processing/papers/3_Generating_Sequences_With_1308-0850v5.pdf), [2](https://github.com/adilsonmedronha/Tutorials/blob/main/Deep%20Learning%20for%20Natural%20Language%20Processing/papers/3_Generating_Text_with_Recurrent_Neural_Networks_LANG-RNN.pdf) \n",
    "\n",
    "4. **Automatic Image Caption Generation** <br>\n",
    "    Generally, the systems involve the use of very large convolutional neural networks for the object detection in the photographs and then a recurrent neural network like an LSTM to turn the labels into a coherent sentence. <br>\n",
    "    Automatic image captioning is the task where given an image the system must generate a caption that describes the contents of the image.\n",
    "    * Papers: [1](https://github.com/adilsonmedronha/Tutorials/blob/main/Deep%20Learning%20for%20Natural%20Language%20Processing/papers/4_Deep_Visual_Semantic_Alignments_for_Generating_Image_Descriptions_cvpr2015.pdf), [2](https://github.com/adilsonmedronha/Tutorials/blob/main/Deep%20Learning%20for%20Natural%20Language%20Processing/papers/4_Explain_Images_with_Multimodal_Recurrent_Neural_Networks_1410-1090v1.pdf) \n",
    "5. **Sentiment analysis** <br>\n",
    "    Aspect specific sentiment analysis using hierarchical deep learning \n",
    "6. **Text classification** <br>\n",
    "    Recurrent Convolutional Neural Networks for Text Classification. <br>\n",
    "    The task is to assign a document to one or more classes or categories <br>\n",
    "    * [Paper](https://github.com/adilsonmedronha/Tutorials/blob/main/Deep%20Learning%20for%20Natural%20Language%20Processing/papers/6_Convolutional_Neural_Networks_For_Text_Classification_9745-44425-1-PB.pdf) \n",
    "7. **Named Entity Recognition** <br>\n",
    "    Neural architectures for named entity recognition. NER — sometimes referred to as entity chunking, <br> extraction, or identification — is the task of identifying and categorizing key information (entities) in text.\n",
    "    * [Paper](https://github.com/adilsonmedronha/Tutorials/blob/main/Deep%20Learning%20for%20Natural%20Language%20Processing/papers/7_Neural_Architectures_for_Named_Entity_Recognition_1603-01360.pdf) \n",
    "8. **Reading Comprehension** <br>\n",
    "    The answer to each question is a segment of text from the corresponding reading passage (Fig. 1) <br> (Stanford Researchers) we build a strong logistic regression model, which achieves an F1 score of 51.0% <br>\n",
    "    However, human performance (86.8%) is much higher.\n",
    "    * [Paper](https://github.com/adilsonmedronha/Tutorials/blob/main/Deep%20Learning%20for%20Natural%20Language%20Processing/papers/8_SQuAD_100%2C000%2B_Questions_for_Machine_Comprehension_of_Text1606-05250.pdf) \n",
    "9. **[InferKit's Text Generation](https://app.inferkit.com/demo)**\n",
    "    It is a tool takes text you provide and generates what it thinks comes next, <br>\n",
    "    using a state-of-the-art neural network. It's configurable and can produce any length of text on <br> practically any topic. An example: <br>\n",
    "    **Input**: While not normally known for his musical talent, Elon Musk is releasing a debut album <br>\n",
    "    **Completion**: While not normally known for his musical talent, Elon Musk is releasing a debut album. <br>\n",
    "    **It called \"The Road to Re-Entry,\" and it features an astounding collection of songs... (continued)** \n",
    "10. **Pixel restoration CSI style** <br>\n",
    "    Early in 2017, Google Brain researchers trained a Deep Learning network to take very low resolution images of faces and predict what each face most likely looks like. <br>\n",
    "    * [Paper](https://github.com/adilsonmedronha/Tutorials/blob/main/Deep%20Learning%20for%20Natural%20Language%20Processing/papers/10_Pixel_Recursive_Super_Resolution_1702-00783.pdf)\n",
    "\n",
    "\n",
    "**Additional Examples** <br>\n",
    "Below are some additional examples to those listed above:\n",
    "\n",
    "* Automatic speech recognition. <br>\n",
    "    [Deep Neural Networks for Acoustic Modeling in Speech Recognition](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/38131.pdf), 2012 <br>\n",
    "* Automatic speech understanding. <br>\n",
    "    [Towards End-to-End Speech Recognition with Recurrent Neural Networks](http://proceedings.mlr.press/v32/graves14.pdf), 2014 <br>\n",
    "* Automatically focus attention on objects in images. <br>\n",
    "    [Recurrent Models of Visual Attention](https://arxiv.org/pdf/1406.6247v1.pdf), 2014 <br>\n",
    "* Automatically answer questions about objects in a photograph. <br>\n",
    "    [Exploring Models and Data for Image Question Answering](https://arxiv.org/pdf/1505.02074v4.pdf), 2015 <br>\n",
    "* Automatically turing sketches into photos. <br>\n",
    "    [Convolutional Sketch Inversion](https://arxiv.org/pdf/1606.03073.pdf), 2016 <br>\n",
    "* Automatically create stylized images from rough sketches.  <br>\n",
    "    [Neural Doodle](https://github.com/alexjc/neural-doodle) <br>\n",
    "\n",
    "**Here’s 30 impressive NLP applications using deep learning methods:** <br>\n",
    "    [It is a map with links](https://www.xmind.net/m/AEYf/). Download this and open it on XMind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='02'></a>\n",
    "## Cleaning Text Data ##\n",
    "\n",
    "**Text is Messy**: you cannot go straight from raw text to fitting a ML or DP model \n",
    "\n",
    "You must clean your text first, which means splitting it into words and normalizing issues such as:\n",
    "* Upper and lower case characteres.\n",
    "* Punctuation within and around words.\n",
    "* Numbers such as amounts and dates.\n",
    "* Spelling mistakes and regional variations.\n",
    "* Unicode characters.\n",
    "* and much more...\n",
    "\n",
    "**Manual Tokenization**\n",
    "\n",
    "* **Tokenization**: turning raw text into something we can model (data structure)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be a filename ...\n",
    "data = \"For God so loved the world that he gave his one and only Son, that whoever believes in him shall not perish but have eternal life\"\n",
    "words = data.split()\n",
    "words = [word.lower() for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK Tokenization**\n",
    "Many of the best practices for tokenizing raw text have been captured <br> \n",
    "and made available in a Python library called the Natural Language Toolkit or NLTK for short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk \n",
    "# nltk.download()\n",
    "# or via a command line:\n",
    "!python -m nltk.downloader all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"datasets\\Bible_KJ.txt\"\n",
    "file = open(filename, \"rt\", encoding='utf-8')\n",
    "text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing \n",
    "* **Stop words** is a commonly used word (such as “the”, “a”, “an”, “in”,...) <br>\n",
    "    We would not want these words to take up space in our database  \n",
    "* **Stemming** is the process of reducing inflected (or sometimes derived) words to their word stem. <br>\n",
    "    *E.g. the words [consultant, consulting, consultantive, consulting] need to be read as **consult**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " before pre-processing:\n",
      " Size = 952469 | #Stop Words = 379601 | Demo = \n",
      "['The', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'King', 'James', 'Bible', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.', 'You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're-use', 'it'] \n",
      "\n",
      " after pre-processing:\n",
      " Size = 376479 | #Stop Words = 176 | Demo = \n",
      "['project', 'gutenberg', 'ebook', 'king', 'jame', 'bibl', 'ebook', 'use', 'anyon', 'anywher', 'unit', 'state', 'part', 'world', 'cost', 'almost', 'restrict', 'whatsoev', 'may', 'copi', 'give', 'away', 'term', 'project', 'gutenberg', 'licens', 'includ', 'ebook', 'onlin', 'locat', 'unit', 'state', 'check', 'law', 'countri', 'locat', 'use', 'ebook', 'titl', 'king', 'jame', 'bibl', 'releas', 'date', 'august', 'ebook', 'recent', 'updat', 'decemb', 'languag'] \n"
     ]
    }
   ],
   "source": [
    "# split into words\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "original = tokens = wt(text)\n",
    "\n",
    "# convert to lower case\n",
    "tokens = [word.lower() for word in tokens]\n",
    "\n",
    "# remove non-alphabetic tokens\n",
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "# filter out stop words\n",
    "#   Stop words are available in abundance in any human language. \n",
    "#   By removing these words, we remove the low-level information \n",
    "#   from our text in order to give more focus to the important information\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "tokens = [word for word in tokens if not word in stopwords]\n",
    "\n",
    "# steamming of words\n",
    "from nltk.stem.porter import PorterStemmer as ps\n",
    "porter = ps()\n",
    "tokens = [porter.stem(word) for word in tokens]\n",
    "\n",
    "cleaned = tokens\n",
    "\n",
    "def to_string(sel = \"before\", data = original):\n",
    "    print(\"\\n {} pre-processing:\\n Size = {} | #Stop Words = {} | Demo = \\n{} \".format(sel, len(data), len([word for word in data if word in stopwords]), data[:50]))\n",
    "\n",
    "to_string()\n",
    "to_string(sel = \"after\", data = cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Information\n",
    "\n",
    "* [Project Gutenberg](https://www.gutenberg.org/) \n",
    "* [nltk.tokenize package API](https://www.nltk.org/api/nltk.tokenize.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='03'></a>\n",
    "## Bag-of-Words Model ##\n",
    "\n",
    "* The bag-of-words model is a way of **representing text** data when modeling text with machine learning algorithms.\n",
    "* A bag-of-words is a representation of text that describes the **occurrence of words** within a document.\n",
    "* It is called a **“bag” of words**, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n",
      "(1, 8)\n",
      "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
      "  0.36388646 0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "\t\t\"The dog.\",\n",
    "\t\t\"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word counts: \n",
      "  OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)]) \n",
      " document count  \n",
      " 5 \n",
      " word index  \n",
      " {'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8} \n",
      " word docs  \n",
      " defaultdict(<class 'int'>, {'well': 1, 'done': 1, 'good': 1, 'work': 2, 'effort': 1, 'great': 1, 'nice': 1, 'excellent': 1})\n",
      "[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# define 5 documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "# summarize what was learned\n",
    "print(\"word counts: \\n  {} \\n document count  \\n {} \\n word index  \\n {} \\n word docs  \\n {}\".format(t.word_counts, t.document_count, t.word_index, t.word_docs ))\n",
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='06'></a>\n",
    "## Classifying Text ##\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Text classification** describes a general class of problems such as predicting <br> the sentiment of tweets and movie reviews, as well as classifying email as spam or not.\n",
    "* **Deep learning methods** are proving very good at text classification,<br> achieving state-of-the-art results on a suite of standard academic benchmark problems.\n",
    "\n",
    "**Embeddings + CNN**\n",
    "The modus operandi for text classification involves the use of a word embedding for  representing words and a Convolutional Neural Network or CNN for learning how to discriminate documents on classification problems.\n",
    "\n",
    "* **Word Embedding Model**: A distributed representation of words where different words that have a similar meaning (based on their usage) also have a similar representation.\n",
    "* **Convolutional Model**: A feature extraction model that learns to extract salient features from documents represented using a word embedding.\n",
    "* **Fully-Connected Model**: The interpretation of extracted features in terms of a predictive output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 200, 100)          10000     \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 193, 32)           25632     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 96, 32)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3072)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                30730     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,373\n",
      "Trainable params: 66,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "# define problem\n",
    "vocab_size = 100\n",
    "max_length = 200\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83cd496450fcf51b91e4243e8b269b065eb05caaf3160c1f0ac565891d082ff9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cat_class_ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
