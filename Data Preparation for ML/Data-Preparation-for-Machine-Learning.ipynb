{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Machine Learning\n",
    "***This 7-day Mini-Course was created by Jason Brownlee <br>*** \n",
    "\n",
    "This crash course is broken down into seven lessons.\n",
    "\n",
    "\n",
    "Below is a list of the seven lessons that will get you started and productive with data <br> preparation in Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lesson 01: [Importance of Data Preparation](#01)\n",
    "* Lesson 02: [Fill Missing Values With Imputation](#02)\n",
    "* Lesson 03: [Select Features With Recursive Feature Elimination (RFE)](#03)\n",
    "* Lesson 04: [Scale Data With Normalization](#04) \n",
    "* Lesson 05: [Transform Categories With One-Hot Encoding](#05)\n",
    "* Lesson 06: [Transform Numbers to Categories With kBins](#06)\n",
    "* Lesson 07: [Dimensionality Reduction with PCA](#07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='01'></a>\n",
    "## Importance of Data Preparation ##\n",
    "* **Predictive modeling** projects involve learning from data\n",
    "* **Data refers** to examples or cases from the domain that characterize the problem you want to solve\n",
    "* On a predictive modeling project, such as classification or regression, **raw data** typically cannot be used directly. \n",
    "\n",
    "There are four main reasons why this is the case:\n",
    "\n",
    "* **Data Types**: ML algorithms require data to be numbers.\n",
    "* **Data Requirements**: Some ML algorithms impose requirements on the data.\n",
    "* **Data Errors**: Statistical noise and errors in the data may need to be corrected.\n",
    "* **Data Complexity**: Complex nonlinear relationships may be teased out of the data.\n",
    "* **Data Preparation**: The **raw data** must be pre-processed prior to being used to fit and evaluate a ML model. \n",
    "\n",
    "Some data preparation methods:\n",
    "\n",
    "\n",
    "* **Standardization** that standardizes the numeric data using the mean and standard deviation of the column.\n",
    "* **Normalization** most often means dividing by a norm of the vector. Scales numerical variables to the range between zero and one. *E.g., divide each pixel value of a image by 255.*\n",
    "* **Filtering** the data if we are interested in phenomenon of particular time or space scale.\n",
    "* **Replacing nan values** with some default values (mean, mode,...).\n",
    "* **Numerical data discretization**: transform numeric data into categorical data. This might be useful when ranges could be more effective than exact values. *E.g., high/medium/low temperatures might be more interesting than the actual temperature.*\n",
    "* **Outlier dectection**: outliers can be noises in terms of finding patterns in datasets. Using boxplot is possible to identify them.\n",
    "* **Principal Component Analysis (PCA)** is used to reduce the dimensionality of data by creating new features. It does this to increase their chances of being interpret-able while minimising information loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='02'></a>\n",
    "## Fill Missing Values With Imputation ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Data imputation**: filling missing values with data. \n",
    "<br> Normally this data is a statistical value (mean, median, frequency,...) of its column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical imputation transform for the horse colic dataset and a full description of dataset can be founded [(here)](https://archive.ics.uci.edu/ml/datasets/Horse+Colic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import isnan \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# load dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv\"\n",
    "dataframe = read_csv(url, header=None, na_values=\"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing before imputation: 1605\n",
      "Missing after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "# Split into input and output elements\n",
    "#   A full description of HC dataset is described in link above.\n",
    "#   Realizes that (in horse colic dataset url description) the range\n",
    "#   from 1 to 28 is attibutes except the number 23 which is the outcome (lived, died or was euthanized) \n",
    "#   reall that: dataframe.shape => (300, 28) \n",
    "data = dataframe.values\n",
    "outcome_col, cols = 23, data.shape[1]\n",
    "ix_cols = [c for c in range(cols) if c != 23]\n",
    "X, y = data[:, ix_cols], data[:, outcome_col]\n",
    "# total missing\n",
    "print(\"Missing before imputation: %d\" %sum(isnan(X).flatten()))\n",
    "# define imputer\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "# fit on the dataset \n",
    "imputer.fit(X)\n",
    "# transform the dataset\n",
    "Xtrans = imputer.transform(X)\n",
    "print(\"Missing after imputation: %d\" % sum(isnan(Xtrans).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data imputation  1605\n",
      "after data imputation  0\n"
     ]
    }
   ],
   "source": [
    "# another technique to do data imputation\n",
    "print(\"before data imputation \", sum(dataframe.isnull().sum()))\n",
    "dataframe = dataframe.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "print(\"after data imputation \", sum(dataframe.isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='03'></a>\n",
    "## Select Features With Recursive Feature Elimination (RFE) ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature ranking with recursive feature elimination: [RFE](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)\n",
    "\n",
    "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), <br>\n",
    "the goal of recursive feature elimination (RFE) is to select features by recursively considering <br> \n",
    "smaller and smaller sets of features. First, the estimator is trained on the initial set of features <br> \n",
    "and the importance of each feature is obtained either through any specific attribute or callable. <br> \n",
    "**Then, the least important features are pruned from current set of features.** That procedure is recursively <br>\n",
    "repeated on the pruned set until the desired number of features to select is eventually reached. <br>\n",
    "\n",
    "* **Basically, RFE works by recursively removing attributes and building a model on those attributes that ramain**\n",
    "\n",
    "* RFE is a transform. To use it, first, the class is configured with the chosen algorithm specified <br> via the “estimator” argument and the number of features to select via the “n_features_to_select” argument. <br>\n",
    "\n",
    "* The example below defines a synthetic classification dataset with five redundant input features. <br>\n",
    "RFE is then used to select five features using the decision tree algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0, Remains? No  => Rank: 5\n",
      "Column: 1, Remains? No  => Rank: 4\n",
      "Column: 2, Remains? Yes => Rank: 1\n",
      "Column: 3, Remains? Yes => Rank: 1\n",
      "Column: 4, Remains? Yes => Rank: 1\n",
      "Column: 5, Remains? No  => Rank: 6\n",
      "Column: 6, Remains? Yes => Rank: 1\n",
      "Column: 7, Remains? No  => Rank: 3\n",
      "Column: 8, Remains? Yes => Rank: 1\n",
      "Column: 9, Remains? No  => Rank: 2\n"
     ]
    }
   ],
   "source": [
    "# report which features were selected by RFE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# define dataset\n",
    "xColumns = 10\n",
    "xRows = 1000\n",
    "X, y = make_classification(n_samples=xRows, n_features=xColumns, n_informative=5, n_redundant=5, random_state=1)\n",
    "# X.shape => (1000, 10)\n",
    "\n",
    "# define RFE\n",
    "# \tn_features_to_select -> #features to keep/remain (not to exclude)\n",
    "#\t.support_ -> return the True and Falses valeus. #Trues = n_features_to_select. \n",
    "#   so, all falses will be removed \n",
    "#   .ranking will show the order of remotion from highest to lowest  [6, 5, 4, 3, 2] rank 1 remains, not deleted\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n",
    "# fit RFE\n",
    "rfe.fit(X, y)\n",
    "# summarize all features\n",
    "\n",
    "def bool_(v):\n",
    "\treturn 'Yes' if v else 'No ' \n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "\tprint('Column: %d, Remains? %s => Rank: %d' % (i, bool_(rfe.support_[i]), rfe.ranking_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "461c46b9803ba338ffc75cdc77e4a9392433d49f705c691da68d2d4e761a221f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv_license_status')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
