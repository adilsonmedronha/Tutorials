{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Machine Learning\n",
    "***This 7-day Mini-Course was created by Jason Brownlee <br>*** \n",
    "\n",
    "This crash course is broken down into seven lessons.\n",
    "\n",
    "\n",
    "Below is a list of the seven lessons that will get you started and productive with data <br> preparation in Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lesson 01: [Importance of Data Preparation](#01)\n",
    "* Lesson 02: [Fill Missing Values With Imputation](#02)\n",
    "* Lesson 03: [Select Features With RFE](#03)\n",
    "* Lesson 04: [Scale Data With Normalization](#04) \n",
    "* Lesson 05: [Transform Categories With One-Hot Encoding](#05)\n",
    "* Lesson 06: [Transform Numbers to Categories With kBins](#06)\n",
    "* Lesson 07: [Dimensionality Reduction with PCA](#07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='01'></a>\n",
    "## Importance of Data Preparation ##\n",
    "* **Predictive modeling** projects involve learning from data\n",
    "* **Data refers** to examples or cases from the domain that characterize the problem you want to solve\n",
    "* On a predictive modeling project, such as classification or regression, **raw data** typically cannot be used directly. \n",
    "\n",
    "There are four main reasons why this is the case:\n",
    "\n",
    "* **Data Types**: ML algorithms require data to be numbers.\n",
    "* **Data Requirements**: Some ML algorithms impose requirements on the data.\n",
    "* **Data Errors**: Statistical noise and errors in the data may need to be corrected.\n",
    "* **Data Complexity**: Complex nonlinear relationships may be teased out of the data.\n",
    "* **Data Preparation**: The **raw data** must be pre-processed prior to being used to fit and evaluate a ML model. \n",
    "\n",
    "Some data preparation methods:\n",
    "\n",
    "\n",
    "* **Standardization** that standardizes the numeric data using the mean and standard deviation of the column.\n",
    "* **Normalization** most often means dividing by a norm of the vector. Scales numerical variables to the range between zero and one. *E.g., divide each pixel value of a image by 255.*\n",
    "* **Filtering** the data if we are interested in phenomenon of particular time or space scale.\n",
    "* **Replacing nan values** with some default values (mean, mode,...).\n",
    "* **Numerical data discretization**: transform numeric data into categorical data. This might be useful when ranges could be more effective than exact values. *E.g., high/medium/low temperatures might be more interesting than the actual temperature.*\n",
    "* **Outlier dectection**: outliers can be noises in terms of finding patterns in datasets. Using boxplot is possible to identify them.\n",
    "* **Principal Component Analysis (PCA)** is used to reduce the dimensionality of data by creating new features. It does this to increase their chances of being interpret-able while minimising information loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='02'></a>\n",
    "## Fill Missing Values With Imputation ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Data imputation**: filling missing values with data. \n",
    "<br> Normally this data is a statistical value (mean, median, frequency,...) of its column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical imputation transform for the horse colic dataset and a full description of dataset can be founded [(here)](https://archive.ics.uci.edu/ml/datasets/Horse+Colic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import isnan \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# load dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv\"\n",
    "dataframe = read_csv(url, header=None, na_values=\"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing before imputation: 1605\n",
      "Missing after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "# Split into input and output elements\n",
    "#   A full description of HC dataset is described in link above.\n",
    "#   Realizes that (in horse colic dataset url description) the range\n",
    "#   from 1 to 28 is attibutes except the number 23 which is the outcome (lived, died or was euthanized) \n",
    "#   reall that: dataframe.shape => (300, 28) \n",
    "data = dataframe.values\n",
    "outcome, cols = 23, data.shape[1]\n",
    "ix = [c for c in range(cols) if c != 23]\n",
    "X, y = data[:, ix], data[:, outcome]\n",
    "# total missing\n",
    "print(\"Missing before imputation: %d\" %sum(isnan(X).flatten()))\n",
    "# define imputer\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "# fit on the dataset \n",
    "imputer.fit(X)\n",
    "# transform the dataset\n",
    "Xtrans = imputer.transform(X)\n",
    "print(\"Missing after imputation: %d\" % sum(isnan(Xtrans).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data imputation  1605\n",
      "after data imputation  0\n"
     ]
    }
   ],
   "source": [
    "# another technique to do data imputation\n",
    "print(\"before data imputation \", sum(dataframe.isnull().sum()))\n",
    "dataframe = dataframe.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "print(\"after data imputation \", sum(dataframe.isnull().sum()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "461c46b9803ba338ffc75cdc77e4a9392433d49f705c691da68d2d4e761a221f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv_license_status')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
