{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Machine Learning\n",
    "\n",
    "***This is my notes of 7-day Mini-Course (created by Jason Brownlee) <br>*** *May have some classmates insights*\n",
    "\n",
    "This crash course is broken down into seven lessons.\n",
    "\n",
    "\n",
    "Below is a list of the seven lessons that will get you started and productive with data <br> preparation in Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lesson 01: [Importance of Data Preparation](#01)\n",
    "* Lesson 02: [Fill Missing Values With Imputation](#02)\n",
    "* Lesson 03: [Select Features With Recursive Feature Elimination (RFE)](#03)\n",
    "* Lesson 04: [Scale Data With Normalization](#04) \n",
    "* Lesson 05: [Transform Categories With One-Hot Encoding](#05)\n",
    "* Lesson 06: [Transform Numbers to Categories With kBins](#06)\n",
    "* Lesson 07: [Dimensionality Reduction with PCA](#07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='01'></a>\n",
    "## Importance of Data Preparation ##\n",
    "* **Predictive modeling** projects involve learning from data\n",
    "* **Data refers** to examples or cases from the domain that characterize the problem you want to solve\n",
    "* On a predictive modeling project, such as classification or regression, **raw data** typically cannot be used directly. \n",
    "\n",
    "There are four main reasons why this is the case:\n",
    "\n",
    "* **Data Types**: ML algorithms require data to be numbers.\n",
    "* **Data Requirements**: Some ML algorithms impose requirements on the data.\n",
    "* **Data Errors**: Statistical noise and errors in the data may need to be corrected.\n",
    "* **Data Complexity**: Complex nonlinear relationships may be teased out of the data.\n",
    "* **Data Preparation**: The **raw data** must be pre-processed prior to being used to fit and evaluate a ML model. \n",
    "\n",
    "Some data preparation methods:\n",
    "\n",
    "\n",
    "* **Standardization** that standardizes the numeric data using the mean and standard deviation of the column.\n",
    "* **Normalization** most often means dividing by a norm of the vector. Scales numerical variables to the range between zero and one. *E.g., divide each pixel value of a image by 255.*\n",
    "* **Filtering** the data if we are interested in phenomenon of particular time or space scale.\n",
    "* **Replacing nan values** with some default values (mean, mode,...).\n",
    "* **Numerical data discretization**: transform numeric data into categorical data. This might be useful when ranges could be more effective than exact values. *E.g., high/medium/low temperatures might be more interesting than the actual temperature.*\n",
    "* **Outlier dectection**: outliers can be noises in terms of finding patterns in datasets. Using boxplot is possible to identify them.\n",
    "* **Principal Component Analysis (PCA)** is used to reduce the dimensionality of data by creating new features. It does this to increase their chances of being interpret-able while minimising information loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='02'></a>\n",
    "## Fill Missing Values With Imputation ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Data imputation**: filling missing values with data. \n",
    "<br> Normally this data is a statistical value (mean, median, frequency,...) of its column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical imputation transform for the horse colic dataset and a full description of dataset can be founded [(here)](https://archive.ics.uci.edu/ml/datasets/Horse+Colic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import isnan \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# load dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv\"\n",
    "dataframe = read_csv(url, header=None, na_values=\"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing before imputation: 1605\n",
      "Missing after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "# Split into input and output elements\n",
    "#   A full description of HC dataset is described in link above.\n",
    "#   Realizes that (in horse colic dataset url description) the range\n",
    "#   from 1 to 28 is attibutes except the number 23 which is the outcome (lived, died or was euthanized) \n",
    "#   reall that: dataframe.shape => (300, 28) \n",
    "data = dataframe.values\n",
    "outcome_col, cols = 23, data.shape[1]\n",
    "ix_cols = [c for c in range(cols) if c != 23]\n",
    "X, y = data[:, ix_cols], data[:, outcome_col]\n",
    "# total missing\n",
    "print(\"Missing before imputation: %d\" %sum(isnan(X).flatten()))\n",
    "# define imputer\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "# fit on the dataset \n",
    "imputer.fit(X)\n",
    "# transform the dataset\n",
    "Xtrans = imputer.transform(X)\n",
    "print(\"Missing after imputation: %d\" % sum(isnan(Xtrans).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data imputation  1605\n",
      "after data imputation  0\n"
     ]
    }
   ],
   "source": [
    "# another technique to do data imputation\n",
    "print(\"before data imputation \", sum(dataframe.isnull().sum()))\n",
    "dataframe = dataframe.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "print(\"after data imputation \", sum(dataframe.isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='03'></a>\n",
    "## Select Features With Recursive Feature Elimination (RFE) ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature ranking with recursive feature elimination: [RFE](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)\n",
    "\n",
    "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), <br>\n",
    "the goal of recursive feature elimination (RFE) is to select features by recursively considering <br> \n",
    "smaller and smaller sets of features. First, the estimator is trained on the initial set of features <br> \n",
    "and the importance of each feature is obtained either through any specific attribute or callable. <br> \n",
    "**Then, the least important features are pruned from current set of features.** That procedure is recursively <br>\n",
    "repeated on the pruned set until the desired number of features to select is eventually reached. <br>\n",
    "\n",
    "* **Basically, RFE works by recursively removing attributes and building a model on those attributes that ramain**\n",
    "\n",
    "* **Choose those features that are statistically meaningful to your model.**\n",
    "\n",
    "* RFE is a transform. To use it, first, the class is configured with the chosen algorithm specified <br> via the “estimator” argument and the number of features to select via the “n_features_to_select” argument. <br>\n",
    "\n",
    "* The example below defines a synthetic classification dataset with five redundant input features. <br>\n",
    "RFE is then used to select five features using the decision tree algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0, Remains? No  => Rank: 5\n",
      "Column: 1, Remains? No  => Rank: 4\n",
      "Column: 2, Remains? Yes => Rank: 1\n",
      "Column: 3, Remains? Yes => Rank: 1\n",
      "Column: 4, Remains? Yes => Rank: 1\n",
      "Column: 5, Remains? No  => Rank: 6\n",
      "Column: 6, Remains? Yes => Rank: 1\n",
      "Column: 7, Remains? No  => Rank: 2\n",
      "Column: 8, Remains? Yes => Rank: 1\n",
      "Column: 9, Remains? No  => Rank: 3\n"
     ]
    }
   ],
   "source": [
    "# report which features were selected by RFE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# define dataset\n",
    "xColumns = 10\n",
    "xRows = 1000\n",
    "X, y = make_classification(n_samples=xRows, n_features=xColumns, n_informative=5, n_redundant=5, random_state=1)\n",
    "# X.shape => (1000, 10)\n",
    "\n",
    "# define RFE\n",
    "# n_features_to_select -> #features to keep/remain (not to exclude)\n",
    "# .support_ -> return the True and Falses valeus. #Trues = n_features_to_select. \n",
    "# so, all falses will be removed \n",
    "# .ranking will show the order of remotion from highest to lowest  [6, 5, 4, 3, 2] rank 1 remains, not deleted\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n",
    "# fit RFE\n",
    "rfe.fit(X, y)\n",
    "# summarize all features\n",
    "\n",
    "def bool_(v):\n",
    "\treturn 'Yes' if v else 'No ' \n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "\tprint('Column: %d, Remains? %s => Rank: %d' % (i, bool_(rfe.support_[i]), rfe.ranking_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='04'></a>\n",
    "## Scale Data With Normalization ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization is required when the value of feature ranges among all features are (too) disproportionate, <br>\n",
    "otherwise the feature with the largest range of values (lets say X_l) would overlaps others in terms of its parameters.** <br>\n",
    "\n",
    "Therefore, if parameter of X_l stands out from others for this reason, we have a biased model that might disregard other weights that <br> could be potentially good. <br>\n",
    "\n",
    "*E.g., the range of age feature is suppouse to be (in years) from 0 to 120, but salary can be from 1k to 1.000.000,00k (and beyond) ...* <br> Of course this features (or dataset) need to be normalized.\n",
    "\n",
    "* **Normalization returns the data with all values withing 0 and 1**\n",
    "\n",
    "Given the following data: <br> \n",
    "\n",
    "$X =[1, 2, 3, 4, 5]$ <br>\n",
    "\n",
    "Calculating $X$ normalized: <br>\n",
    "$X_{std} =  \\frac{X - X_{min}}{X_{max} - X_{min}} \\rightarrow X_{std} = \\frac{ [1, 2, 3, 4, 5] - 1}{5 - 1} = \\frac{[0,1,2,3,4]}{4}= \\begin{bmatrix}\n",
    "0 \\\\\n",
    "0.25 \\\\\n",
    "0.5 \\\\ \n",
    "0.75 \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  ]\n",
      " [0.25]\n",
      " [0.5 ]\n",
      " [0.75]\n",
      " [1.  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# define dataset\n",
    "X      = np.array([1,2,3,4,5])\n",
    "# define scaler\n",
    "scaler = MinMaxScaler()\n",
    "# summarize data after the transform\n",
    "print(scaler.fit_transform(X.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5)\n",
      "[[ 2.39324489 -5.77732048 -0.59062319 -2.08095322  1.04707034]\n",
      " [-0.45820294  1.94683482 -2.46471441  2.36590955 -0.73666725]\n",
      " [ 2.35162422 -1.00061698 -0.5946091   1.12531096 -0.65267587]]\n",
      "\n",
      "normalized version of X:\n",
      " [[0.77608466 0.0239289  0.48251588 0.18352101 0.59830036]\n",
      " [0.40400165 0.79590304 0.27369632 0.6331332  0.42104156]\n",
      " [0.77065362 0.50132629 0.48207176 0.5076991  0.4293882 ]]\n"
     ]
    }
   ],
   "source": [
    "# another example of normalizing input data\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_informative=5, n_redundant=0, random_state=1)\n",
    "# summarize data before the transform\n",
    "print(X.shape)\n",
    "print(X[:3, :])\n",
    "# define the scaler\n",
    "trans = MinMaxScaler()\n",
    "# transform the data\n",
    "X_norm = trans.fit_transform(X)\n",
    "# summarize data after the transform\n",
    "print(\"\\nnormalized version of X:\\n\",X_norm[:3, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='05'></a>\n",
    "## Transform Categories With One-Hot Encoding ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, (remember your statistics class) a data can have the following types:\n",
    "* Numerical (quantitative) data\n",
    "    * Numerical values: **discrete** (countable) or **continuous** (measurable: temp., weight, length...)\n",
    "* Categorical (qualitative) data\n",
    "    * Categorical values: **nominal** (gender, colour...) or **ordinal** (if you can order or rank it: always, usually, rarely...)\n",
    "\n",
    "\n",
    "One Hot Encoding is a technique to enconde categorical input variables (**qualitative variable**) as numbers (**continuous/discrete variables**). <br>\n",
    "Remember that machine learning models require all input and output variables to be numeric. This means that if your data <br> \n",
    "contains categorical data, **you must encode it to numbers before you can fit and evaluate a model.** <br>\n",
    "\n",
    "* **Ordinal Encoding:** each label (class) for a categorical variable can be mapped to a unique integer\n",
    "* **One-hot encoding** is the most popular approach to do this transformation\n",
    "\n",
    "For example, imagine we have a “color” variable with three categories (‘red‘, ‘green‘, and ‘blue‘):\n",
    "\n",
    "| Red  | Green  | Blue |   \n",
    "|------|--------|-------|\n",
    "|  1   |   0    |  0    |   \n",
    "|  0   |   1    |  0    |   \n",
    "|  0   |   0    |  1    |  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[\"'40-49'\" \"'premeno'\" \"'15-19'\" \"'0-2'\" \"'yes'\" \"'3'\" \"'right'\"\n",
      "  \"'left_up'\" \"'no'\"]]\n",
      "\n",
      "We had 9 column that its converts in 43 columns after the one-hot encoding process:\n",
      " [[0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode the breast cancer dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import OneHotEncoder as OH\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv\"\n",
    "# load dataset\n",
    "dataset = read_csv(url, header=None)\n",
    "# retrieve the array of data\n",
    "data = dataset.values\n",
    "# separate into input and output columns\n",
    "# It is a good idea to transform inputs and outputs separately,\n",
    "# so you can invert the transform later separately for predictions.\n",
    "X = data[:, :-1].astype(str)\n",
    "y = data[:,  -1].astype(str)\n",
    "# summarize the raw data\n",
    "print(\"\\n\", X[:1, :])\n",
    "# define the one hot encoding transform\n",
    "encoder = OH(sparse=False)\n",
    "# fit and apply the transform to the input data\n",
    "X_oe = encoder.fit_transform(X)\n",
    "# summarize the transformed data\n",
    "print(\"\\nWe had 9 column that its converts in 43 columns after the one-hot encoding process:\\n\", X_oe[:1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 1. 0. 2. 2. 1. 2. 1.]]\n",
      "[[2. 0. 1. 0. 2. 2. 1. 2. 0.]]\n",
      "[[0. 0. 1. 0. 2. 2. 1. 2. 0.]]\n",
      "[[3. 1. 1. 0. 2. 2. 1. 2. 0.]]\n",
      "[[4. 1. 1. 0. 2. 2. 1. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# here, instead of one-hot encondig, ordinal enconder is doing the work\n",
    "# you can see that OE doesnt just use 1s and 0s to represent the encoded such as one-hot encoding\n",
    "# e.g., \"70-79\" is represented as digit 4, whenever there is 4 in that position (COLUMN), it will be \"70-79\"\n",
    "from sklearn.preprocessing import OrdinalEncoder as OE\n",
    "enc = OE()\n",
    "X = [['40-49', 'premeno', '15-19', '0-2', 'yes', '3', 'right', 'left_up', 'no'], \n",
    "    ['50-59', 'ge40', '15-19', '0-2', 'no', '1', 'right', 'centra', 'yes'], \n",
    "    ['30-39', 'premeno', '30-34', '6-8', 'yes', '2', 'right', 'right_up', 'no'], \n",
    "    ['60-69', 'lt40', '10-14', '0-2', 'no', '1', 'left', 'right_up', 'no'], \n",
    "    ['70-79', 'ge40', '15-19', '9-11', 'nan', '1', 'left', 'left_low', 'yes']]\n",
    "enc.fit(X)\n",
    "\n",
    "print(enc.transform([[\"40-49\", \"premeno\", '15-19', '0-2', 'yes', '3', 'right', 'left_up', 'yes']]))\n",
    "print(enc.transform([[\"50-59\", \"ge40\", '15-19', '0-2', 'yes', '3', 'right', 'left_up', 'no']]))\n",
    "print(enc.transform([[\"30-39\", \"ge40\", '15-19', '0-2', 'yes', '3', 'right', 'left_up', 'no']]))\n",
    "print(enc.transform([[\"60-69\", \"lt40\", '15-19', '0-2', 'yes', '3', 'right', 'left_up', 'no']]))\n",
    "print(enc.transform([[\"70-79\", \"lt40\", '15-19', '0-2', 'yes', '3', 'right', 'left_up', 'no']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the length of encoded data by one-hot encoding may be longer than the raw ones due to the limitation of binary representation <br>\n",
    "However, in ordinals, if there are 9 columns in raw data, the transformed ones will also have 9 columns as does not use only <br>\n",
    "1 and 0, but 1,2,3... as many numbers as needed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='06'></a>\n",
    "## Transform Numbers to Categories With kBins ##\n",
    "\n",
    "In this lesson, you will discover how to **transform numerical variables into categorical variables**.\n",
    "* Some machine learning algorithms may prefer or require categorical or ordinal input variables, <br> such as some decision tree and rule-based algorithms.\n",
    "* One approach is to use the transform of the numerical variable to have a discrete probability <br> distribution where each numerical value is assigned a label and the labels have an ordered (ordinal) relationship.\n",
    "* This is called a **discretization transform** and can improve the performance of some ML models <br> for datasets by making the probability distribution of numerical input variables discrete.\n",
    "\n",
    "The discretization transform is available in the scikit-learn Python machine learning library via the [KBinsDiscretizer class](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.39324489 -5.77732048 -0.59062319 -2.08095322  1.04707034]\n",
      " [-0.45820294  1.94683482 -2.46471441  2.36590955 -0.73666725]\n",
      " [ 2.35162422 -1.00061698 -0.5946091   1.12531096 -0.65267587]]\n",
      "[[7. 0. 4. 1. 5.]\n",
      " [4. 7. 2. 6. 4.]\n",
      " [7. 5. 4. 5. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# discretize numeric input variables\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_informative=5, n_redundant=0, random_state=1)\n",
    "# summarize data before the transform\n",
    "print(X[:3, :])\n",
    "# define the transform\n",
    "trans = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "# transform the data\n",
    "X_discrete = trans.fit_transform(X)\n",
    "# summarize data after the transform\n",
    "print(X_discrete[:3, :])\n",
    "\n",
    "# The raw data is spread in between lowest value -5.77 to maximum 2.39. \n",
    "# Now after transform, it is like binning the data in classes 0-7.\n",
    "# So we see that lowest value is put as class 0 and highest value as class 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='07'></a>\n",
    "# Dimensionality Reduction with PCA ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of input variables or features for a dataset is referred to as its dimensionality \n",
    "\n",
    "* **#features** = dimensionality\n",
    "* **Dimensionality reduction techniques** = reduce #features in a dataset\n",
    "* **More input features** = more challenging for model to predict \n",
    "* **Curse of dimensionality** = more generally \n",
    "* **Principal Component Analysis (PCA)** is the most popular technique for dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below creates a synthetic binary classification dataset with 10 input variables then uses PCA to reduce the dimensionality of the dataset to the three most important components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( the original size =  30 )\n",
      " [[-0.53448246  0.93837451  0.38969914  0.0926655   1.70876508  1.14351305\n",
      "  -1.47034214  0.11857673 -2.72241741  0.2953565 ]\n",
      " [-2.42280473 -1.02658758 -2.34792156 -0.82422408  0.59933419 -2.44832253\n",
      "   0.39750207  2.0265065   1.83374105  0.72430365]\n",
      " [-1.83391794 -1.1946668  -0.73806871  1.50947233  1.78047734  0.58779205\n",
      "  -2.78506977 -0.04163788 -1.25227833  0.99373587]]\n",
      "\n",
      "( the new size =  9 )\n",
      " [[-1.64710578 -2.11683302  1.98256096]\n",
      " [ 0.92840209  4.8294997   0.22727043]\n",
      " [-3.83677757  0.32300714  0.11512801]]\n"
     ]
    }
   ],
   "source": [
    "# example of PCA for dimensionality reduction\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA \n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=3, n_redundant=7, random_state=1)\n",
    "# summarize data before the transform\n",
    "print(\"( the original size = \", len(X[:3, :][0])*len(X[:3, :]), \")\\n\", X[:3, :])\n",
    "# define the transform\n",
    "transf = PCA(n_components=3)\n",
    "# transform the data\n",
    "X_dim = transf.fit_transform(X)\n",
    "# summarize data after the transform \n",
    "print(\"\\n( the new size = \", len(X_dim[:3, :][0])*len(X_dim[:3, :]), \")\\n\", X_dim[:3, :])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "461c46b9803ba338ffc75cdc77e4a9392433d49f705c691da68d2d4e761a221f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv_license_status')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
